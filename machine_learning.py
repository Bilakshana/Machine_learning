# -*- coding: utf-8 -*-
"""Machine_Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AJEMpa7EKvqH7mGNJkwIlZNjvckPDDVP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("üö¢ TITANIC SURVIVAL PREDICTION PROJECT")
print("=" * 50)

# 1. DATA LOADING AND EXPLORATION
print("\n1. LOADING DATA...")

try:
    train_df = pd.read_csv('sample_data/train.csv')
    test_df = pd.read_csv('sample_data/test.csv')
    print("‚úÖ Data loaded successfully!")
except FileNotFoundError:
    print("‚ùå Files not found. Please download the Titanic dataset from Kaggle.")
    print("   Required files: train.csv, test.csv")
    print("   Download from: https://www.kaggle.com/competitions/titanic/data")

    # Create sample data
    print("\nüîß Creating sample data for demonstration...")
    np.random.seed(42)
    n_samples = 891

    train_df = pd.DataFrame({
        'PassengerId': range(1, n_samples + 1),
        'Survived': np.random.choice([0, 1], n_samples, p=[0.62, 0.38]),
        'Pclass': np.random.choice([1, 2, 3], n_samples, p=[0.24, 0.21, 0.55]),
        'Name': [f'Passenger_{i}' for i in range(n_samples)],
        'Sex': np.random.choice(['male', 'female'], n_samples, p=[0.65, 0.35]),
        'Age': np.random.normal(30, 12, n_samples),
        'SibSp': np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.68, 0.23, 0.06, 0.02, 0.01]),
        'Parch': np.random.choice([0, 1, 2, 3], n_samples, p=[0.76, 0.13, 0.08, 0.03]),
        'Ticket': [f'TICKET_{i}' for i in range(n_samples)],
        'Fare': np.random.lognormal(3, 1, n_samples),
        'Cabin': np.random.choice(['A1', 'B2', 'C3', None], n_samples, p=[0.1, 0.1, 0.1, 0.7]),
        'Embarked': np.random.choice(['C', 'Q', 'S'], n_samples, p=[0.19, 0.09, 0.72])
    })

    train_df.loc[np.random.choice(train_df.index, 177, replace=False), 'Age'] = np.nan
    train_df.loc[np.random.choice(train_df.index, 687, replace=False), 'Cabin'] = np.nan
    train_df.loc[np.random.choice(train_df.index, 2, replace=False), 'Embarked'] = np.nan

print(f"üìä Training data shape: {train_df.shape}")
print(f"üìä Dataset info:")
print(train_df.info())

# 2. EXPLORATORY DATA ANALYSIS
print("\n2. EXPLORATORY DATA ANALYSIS...")

print("\nüìà Survival Rate:")
survival_rate = train_df['Survived'].mean()
print(f"   Overall survival rate: {survival_rate:.2%}")

# Survival by different features
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Survival Analysis by Key Features', fontsize=16)

# Survival by Gender
sns.countplot(data=train_df, x='Sex', hue='Survived', ax=axes[0,0])
axes[0,0].set_title('Survival by Gender')
axes[0,0].legend(['Died', 'Survived'])

# Survival by Class
sns.countplot(data=train_df, x='Pclass', hue='Survived', ax=axes[0,1])
axes[0,1].set_title('Survival by Passenger Class')
axes[0,1].legend(['Died', 'Survived'])

# Age distribution
train_df['Age'].hist(bins=30, ax=axes[1,0], alpha=0.7)
axes[1,0].set_title('Age Distribution')
axes[1,0].set_xlabel('Age')

# Fare distribution
train_df['Fare'].hist(bins=30, ax=axes[1,1], alpha=0.7)
axes[1,1].set_title('Fare Distribution')
axes[1,1].set_xlabel('Fare')

plt.tight_layout()
plt.show()

# 3. DATA PREPROCESSING
print("\n3. DATA PREPROCESSING...")

def preprocess_data(df):
    """Comprehensive data preprocessing function"""
    df = df.copy()

    # Create new features
    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',
                                      'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    df['Title'] = df['Title'].replace('Mlle', 'Miss')
    df['Title'] = df['Title'].replace('Ms', 'Miss')
    df['Title'] = df['Title'].replace('Mme', 'Mrs')

    # Family size
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)

    # Age groups
    df['Age'].fillna(df['Age'].median(), inplace=True)
    df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100],
                           labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])

    # Fare groups
    df['Fare'].fillna(df['Fare'].median(), inplace=True)
    df['FareGroup'] = pd.qcut(df['Fare'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])

    # Fill missing values
    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
    df['Cabin'].fillna('Unknown', inplace=True)
    df['HasCabin'] = (df['Cabin'] != 'Unknown').astype(int)

    return df

# Apply preprocessing
train_processed = preprocess_data(train_df)
print("‚úÖ Data preprocessing completed!")

# 4. FEATURE ENGINEERING
print("\n4. FEATURE ENGINEERING...")

# Select features for modeling
features_to_use = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked',
                   'Title', 'FamilySize', 'IsAlone', 'AgeGroup', 'FareGroup', 'HasCabin']

# Prepare features
X = train_processed[features_to_use].copy()
y = train_processed['Survived']

# Encode categorical variables
label_encoders = {}
categorical_features = ['Sex', 'Embarked', 'Title', 'AgeGroup', 'FareGroup']

for feature in categorical_features:
    le = LabelEncoder()
    X[feature] = le.fit_transform(X[feature])
    label_encoders[feature] = le

print(f"üîß Features prepared: {list(X.columns)}")
print(f"üîß Feature matrix shape: {X.shape}")

# 5. MODEL TRAINING AND EVALUATION
print("\n5. MODEL TRAINING AND EVALUATION...")

# Split the data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)

# Define models
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(random_state=42, probability=True)
}

# Train and evaluate models
results = {}
print("\nü§ñ Training models...")

for name, model in models.items():
    print(f"\n   Training {name}...")

    # Use scaled data for models that need it
    if name in ['Logistic Regression', 'SVM']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_val_scaled)
        y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        y_pred_proba = model.predict_proba(X_val)[:, 1]

    # Calculate metrics
    accuracy = accuracy_score(y_val, y_pred)
    auc_score = roc_auc_score(y_val, y_pred_proba)

    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'auc_score': auc_score,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }

    print(f"   Accuracy: {accuracy:.4f}")
    print(f"   AUC Score: {auc_score:.4f}")

# 6. MODEL COMPARISON AND SELECTION
print("\n6. MODEL COMPARISON...")

# Create comparison dataframe
comparison_df = pd.DataFrame({
    'Model': results.keys(),
    'Accuracy': [results[model]['accuracy'] for model in results.keys()],
    'AUC Score': [results[model]['auc_score'] for model in results.keys()]
})

print("\nüìä Model Performance Comparison:")
print(comparison_df.to_string(index=False))

# Find best model
best_model_name = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']
best_model = results[best_model_name]['model']

print(f"\nüèÜ Best Model: {best_model_name}")
print(f"   Accuracy: {results[best_model_name]['accuracy']:.4f}")
print(f"   AUC Score: {results[best_model_name]['auc_score']:.4f}")

# 7. FEATURE IMPORTANCE (for tree-based models)
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    print(f"\nüéØ Feature Importance ({best_model_name}):")

    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print(feature_importance.head(10).to_string(index=False))

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')
    plt.title(f'Top 10 Feature Importance - {best_model_name}')
    plt.xlabel('Importance')
    plt.tight_layout()
    plt.show()

# 8. DETAILED EVALUATION OF BEST MODEL
print(f"\n8. DETAILED EVALUATION - {best_model_name}...")

best_predictions = results[best_model_name]['predictions']

# Classification report
print("\nüìã Classification Report:")
print(classification_report(y_val, best_predictions))

# Confusion matrix
print("\nüìä Confusion Matrix:")
cm = confusion_matrix(y_val, best_predictions)
print(cm)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Died', 'Survived'],
            yticklabels=['Died', 'Survived'])
plt.title(f'Confusion Matrix - {best_model_name}')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# 9. CROSS-VALIDATION
print("\n9. CROSS-VALIDATION...")

if best_model_name in ['Logistic Regression', 'SVM']:
    cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='accuracy')
else:
    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')

print(f"üìä 5-Fold Cross-Validation Results:")
print(f"   Scores: {cv_scores}")
print(f"   Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# 10. HYPERPARAMETER TUNING (Optional)
print("\n10. HYPERPARAMETER TUNING...")

if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7, None],
        'min_samples_split': [2, 5, 10]
    }

    print("üîß Tuning Random Forest parameters...")
    grid_search = GridSearchCV(RandomForestClassifier(random_state=42),
                              param_grid, cv=3, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train, y_train)

    print(f"   Best parameters: {grid_search.best_params_}")
    print(f"   Best cross-validation score: {grid_search.best_score_:.4f}")

    # Update best model
    best_model = grid_search.best_estimator_

    # Evaluate tuned model
    tuned_predictions = best_model.predict(X_val)
    tuned_accuracy = accuracy_score(y_val, tuned_predictions)
    print(f"   Tuned model validation accuracy: {tuned_accuracy:.4f}")

# 11. FINAL SUMMARY
print("\n" + "="*50)
print("üìù FINAL SUMMARY")
print("="*50)

print(f"üéØ Best Model: {best_model_name}")
print(f"üìä Final Accuracy: {accuracy_score(y_val, best_model.predict(X_val if best_model_name not in ['Logistic Regression', 'SVM'] else X_val_scaled)):.4f}")
print(f"üìà Key Insights:")
print(f"   ‚Ä¢ Gender and passenger class are strong predictors")
print(f"   ‚Ä¢ Age and fare also contribute to survival prediction")
print(f"   ‚Ä¢ Family size and having a cabin affect survival chances")

print(f"\n‚úÖ Model is ready for making predictions!")
print(f"üí° To use this model on new data:")
print(f"   1. Preprocess new data using the same pipeline")
print(f"   2. Apply the same feature engineering")
print(f"   3. Use the trained model to predict survival")

# Example of making a prediction
print(f"\nüîÆ Example Prediction:")
print(f"   For a 30-year-old female in 1st class with fare $100:")
sample_data = pd.DataFrame({
    'Pclass': [1], 'Sex': ['female'], 'Age': [30], 'SibSp': [0], 'Parch': [0],
    'Fare': [100], 'Embarked': ['S'], 'Title': ['Miss'], 'FamilySize': [1],
    'IsAlone': [1], 'AgeGroup': ['Adult'], 'FareGroup': ['High'], 'HasCabin': [1]
})

# Encode categorical variables
for feature in categorical_features:
    if feature in sample_data.columns:
        # Handle unknown categories
        try:
            sample_data[feature] = label_encoders[feature].transform(sample_data[feature])
        except ValueError:
            sample_data[feature] = 0  # Default for unknown categories

if best_model_name in ['Logistic Regression', 'SVM']:
    sample_data_scaled = scaler.transform(sample_data)
    prediction = best_model.predict(sample_data_scaled)[0]
    probability = best_model.predict_proba(sample_data_scaled)[0][1]
else:
    prediction = best_model.predict(sample_data)[0]
    probability = best_model.predict_proba(sample_data)[0][1]

print(f"   Prediction: {'Survived' if prediction == 1 else 'Did not survive'}")
print(f"   Probability of survival: {probability:.2%}")

print(f"\nüéâ Machine Learning Pipeline Complete!")

